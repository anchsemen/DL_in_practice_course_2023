## Отчет
### *By Семён Кирсанов, Данил Кладницкий, Никита Ценцеря, Виктор Пахолков*

<a href="https://drive.google.com/drive/folders/1axNEKzSzN3GnZfIaerYji0VFf2v2rN2_?usp=drive_link">Ссылка на векторизованные данные</a>
### Данные
Для создания модели были использованы данные из двух источников, предложенных для кейса: резюме были взяты из датасета резюме HeadHunter, в то время как вакансии были взяты из открытых данных от "Работы России".


Структура датасета от и от "Работы России", и от hh.ru предполагает большое количество колонок, которые по большей мере пустые и дают мало информации для нашей цели, поэтому была использована только часть данных.
Чтобы сохранить компактность обучающих данных, были с "Работы России" было выбрано 2 датасета - вакансии из Сибирского Федерального Округа и города Байконур.

В области предобработки датасетов вакансий, первоначально были оставленны наиболее существенные и заполненные колонки - 'vacancy_name', 'busy_type', 'schedule_type', 'education', 'education_speciality', 'position_requirements', 'position_responsibilities', 'regionName', 'required_certificates',  'required_experience', 'salary_max', 'professionalSphereName'. 
Остальные же колонки были убраны из датасета, по причине их малой заполненности и нерелевантности для задачи. После этого для вакансий были удалены HTML тэги, а для корректной работы моделей, колнки с NA были заполненны прочерком. 
Резюме же изначально шли в куда более удобной и чистой форме, поэтому на данном этапе для них были отобраны колонки - 'Ищет работу на должность:', 'Занятость', 'График', 'Образование и ВУЗ', 'Город', 'Переезд', 'Командировки', 'Опыт работы', 'ЗП', 'Последнее/нынешнее место работы', 'Последняя/нынешняя должность'.

По итогу получилось 44744 резюме и 108577 вакансий.

Однако для последущей работы моделей вследствие ограниченности ресурсов и достаточности для проверки на адекватность работы сервиса, а также удобства деплоя, были засемплированны 10000 вакансий и 10000 резюмею

Далее была проведена обработка текста. Текста вакансии и резюме были токенезированы для удобства лемматизации, затем лемматизированны с помощью функции MorphAnalyzer() из библиотеки pymorphy2.
Pymorphy2 – библиотека для морфологического анализа русского текста на естественном языке. 
Лемматизация текстов резюме и вакансий является важным этапом для создания системы мэтчинга, поскольку она позволяет сопоставить слова в различных формах к их базовой форме (лемме). Это помогает учесть различные формы одного и того же слова при анализе текстов.
Pymorphy2 является одной из наиболее популярных и широко используемых библиотек для морфологического анализа русского текста. Ее словарь обширен и хорошо поддерживается сообществом. Она предоставляет стабильные и точные результаты лемматизации.
Также были удалены стоп-слова.
После этого слова снова были объеденины в колонку текст для предобработки токенизатором соответствующей модели.

### Модели

Для векторизации текстов были использованы модели, основанные на GPT2 и GPT3 для русского языка - семейство моделей rugpt3 based on gpt2 от AI Forever и ruBert-base от тех же AI Forever совместно с SberDevices.
Семейство моделей-трансформеров rugpt3_based_on_gpt2 от AI forever представляет собой набор моделей, основанных на архитектуре GPT-2. 
Эти модели были обучены на массивном наборе данных текстов и кода, что позволило им стать заметными в области задачи векторизации текстов и создания эмбеддингов.
Они обладают высокой способностью к обобщению и извлечению семантической информации из текста. 
Это позволяет им создавать векторные представления текстов вакансий и резюме, которые отражают их смысловое содержание.
Они способны обрабатывать длинные текстовые фрагменты. 
Это важно для задачи мэтчинга вакансий и резюме, поскольку эти документы могут содержать большое количество информации, относящейся к требуемым навыкам и опыту кандидата.

ruBert-base представляет собой трансформерную языковую модель, предварительно обученную на большом наборе данных русского текста. Она способна понимать и обрабатывать естественный язык, что позволяет ей учитывать контекст и семантику вакансий и резюме. Она имеет большую размерность, что позволяет ей генерировать более точные и подробные векторы. Она была обучена на наборе данных русского текста, что позволяет ей лучше понимать и обрабатывать русский язык.

### Рассматривались 3 вариации одного типа моделей и 1 другая модель:

 - **rugpt3small_based_on_gpt2**
Модель была предварительно обучена с длиной последовательности 1024 с использованием библиотеки transformers командой SberDevices на объеме данных около 80 миллиардов токенов 3 эпохи.
Затем модель была отфайнтьюнена с размером контекста 2048.
https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2
   
 - **rugpt3medium_based_on_gpt2**
Модель была предварительно обучена с длиной последовательности 1024 с использованием библиотеки Transformers командой SberDevices на 80 миллиардах токенов в течение 3 эпох.
Затем модель была отфайнтьюнена с размером контекста в 2048 токенов.
Окончательная perplexity на тестовом наборе данных составила 17.4.
https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2
  
 - **rugpt3large_based_on_gpt2**
Модель была обучена с длиной последовательности 1024 с использованием библиотеки Transformers командой SberDevices на 80 миллиардов токенов в течение 3 эпох.
Затем модель была отфайнтьюнена 1 эпоху с длиной последовательности 2048.
Окончательная perplexity на тестовом наборе данных составила 13.6.
https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2

- **ruBert-base**
Модель была обучена командой SberDevices с BPE токенайзером и размерностью словаря в 120138. Финальная модель содержит 178М параметров. Объем тестовых данных составлял 30 ГБ.
Длина последоватльности составляла 512.
Общее число шагов тренировки - 10^6.
https://huggingface.co/ai-forever/ruBert-base



### Проведенные эксперименты:

Для каждой модели прилагающийся к ней tokenizer и  сама модель были загружены с huggingface. 
После этого, текста были токенезированы с помощью нативного токенайзера модели, а затем векторезованны.

После этого были написаны функции для расчета косинусной похожести между векторами резюме и вакансий, а так же евклидова расстояния, однаго основной метрикой явлется именно косинусная похожесть.

Cosine similarity было выбрано по нескольким причинам.
Cosine similarity не зависит от длины текстов и фокусируется на направлении векторов. Это особенно важно для текстов вакансий и резюме, которые могут иметь различную длину.
В многомерных пространствах, где каждое слово или терм представляет собой отдельную размерность, векторы могут быть разреженными. 
Cosine similarity хорошо работает с разреженными данными, что упрощает работу с текстами.
Кроме того cosine similarity хорошо интерпретируем и широко используется в области подобных NLP задач.

Эксперименты таким образом были проведены для трех моделей, в рамках которых сначала случайная вакансия использовался как референс вектор, потом случайное резюме использовалось как референс вектор.
Для полученной таким образом векторизованной вакансии/резюме выделялось 5 наиболее подходящий по косиносному сходству резюме или вакансий соответственно.
Процедура повторялась 5 раз. 

### Результаты:

Вследствие того, что резюме и вакансии были взяты из разных источников, а кроме того не всегда названия должностей в оных совпадали друг с другом, применение каких-либо строгих метрик показалось нам затруднительным.

Мы с командой решили провести проверку на адекватность, оценивая самостоятельно соответствие предлагаемых сервисом на основе косинусной схожести названий резюме задаваемому названию вакансии
и предлагаемых названий вакансий заданному названию резюме.

![gptsmall1](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/6ce4fd16-c7d6-493e-85f6-14281e3b912e)97db-41cd-a0b7-89fecd0f0cf2)
![gptsmall2](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/d68daa6a-80d0-419d-8d23-f1506885bc34)

Figs. 1 - 2 Результаты работы rugpt3small_based_on_gpt2

![gptmedium1](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/a7b310b2-3faf-44f3-af61-6c5154362a9b)
![gptmedium2](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/738e03f5-984b-4011-9f78-8b77dc95548f)

Figs. 3-4 Результаты работы rugpt3medium_based_on_gpt2

![gptlarge1](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/60881dff-9893-446e-b3eb-c5e3ebacf2c7)
![gptlarge2](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/48a266ee-5df4-43e0-909c-c41941dad873)

Fig 5-6. Результаты работы rugpt3large_based_on_gpt2

![bert1](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/123a5d07-96c9-4649-947d-41281b76d2f1)
![bert2](https://github.com/danilkladnitsky/computer-vision-course-2023/assets/56613496/695510d7-5dc3-460f-a5ca-1dc3ac855edd)

Fig 6-7. ruBert-base

Таким образом, результат работы rugpt3large_based_on_gpt2 всем членам команды показался наилучшим образом совпадющем с референс вектором, кроме того, именно благодаря этой модели мы получили наивысшие результаты 
cosine similarity, поэтому именно ее было принято решение использовать в дальнейшем в рамках сервиса.

Кроме того, мы опирались на статью "A Family of Pretrained Transformer Language Models for Russian"б препринт которой был выложен коллективом авторов на arxiv (https://arxiv.org/abs/2309.10931) где разбирались результаты тестировки разных моделей на разном наборе задач, и rugpt3large_based_on_gpt2 авторами была отмечена как одна из лучших моделей для векторизациии, в то время как ruBert-base показывает лучшие результаты в задачах векторизации для последующей классификации текстов.

Система может использоваться для разных сценариев.

Она может быть использована в качестве рекомендательной системы для соискателей, помогая им находить подходящие вакансии на основе анализа схожести их резюме с описаниями вакансий. 
Это может значительно упростить процесс поиска работы, сокращая время, затрачиваемое соискателями на поиск подходящих вакансий.

Работодатели могут использовать эту систему для оценки совместимости между вакансией и кандидатами, позволяя им быстро и эффективно находить наилучших соискателей для определенной роли. 
Это может улучшить качество подбора персонала и сделать процесс менее трудозатратным.

Система может использоваться в HR-процессах для автоматизации предварительного отбора кандидатов. 
Автоматизированная оценка схожести между резюме и вакансиями поможет значительно сэкономить время, которое HR-специалисты тратят на первичный анализ больших объемов данных.

Ваша система может предоставлять аналитику о том, насколько хорошо размещенные вакансии соответствуют потребностям соискателей. 
Это может быть полезным для компаний, чтобы оптимизировать формулировку и контент вакансий для привлечения большего числа подходящих кандидатов.

Проблем с оптимизацией модели и масштабируемостью возникнуть не должно, вследствие относительной быстроты процесса векторизации одной вакансии/одного резюме.


### Инструкция по развертыванию
#### Telegram Gateway
Перед запуском заполнить файл .env в папке telegram-gateway:
```
BOT_PORT=<Порт бота, по умолчанию 3000>
BOT_TOKEN=<Токен созданного бота из Bot Father>
ML_HOST_URL=<Адрес эндпоинта ML-сервиса, по умолчанию: http://localhost:8000/detection-response>

```

Далее в этой же папке нужно выполнить следующий набор команд 
```
npm i
npm run dev
```

#### ML Service (в отдельном терминале)
```
uvicorn main:app --port 8000
```

После всего нужно зайти в бота в телеграме и отправить ему pdf-файл вакансии.
